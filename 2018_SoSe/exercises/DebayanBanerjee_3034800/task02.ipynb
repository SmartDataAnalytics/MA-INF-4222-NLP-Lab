{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lab exercise 2\n",
    "### Jing-Long Wu / Mtr. Nr. 3045999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "df.shape\n",
    "df.head()\n",
    "y = df.label\n",
    "df = df.drop('label', axis=1)\n",
    "\n",
    "x=df['text']\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(x))\n",
    "x_raw_train=x[:train_size]\n",
    "x_raw_test=x[train_size:]\n",
    "\n",
    "y_raw_train=y[:train_size]\n",
    "y_raw_test=y[train_size:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train measurment\n",
      "precision recall Fmeasure accuracy\n",
      "[0.9101450471676342, 0.9083168771555759, 0.9079029301777306, 0.9079825310637952]\n"
     ]
    }
   ],
   "source": [
    "x=df['text']\n",
    "train_size = int(0.7 * len(x))\n",
    "x_raw_train=x[:train_size]\n",
    "x_raw_test=x[train_size:]\n",
    "\n",
    "y_raw_train=y[:train_size]\n",
    "y_raw_test=y[train_size:]\n",
    "\n",
    "acc_score,fmeasure,pre_score,rcall_score=0,0,0,0\n",
    "fold_size=5\n",
    "performance_measure_model1=[]\n",
    "# vectorizer test data for 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=fold_size, shuffle=True, random_state=0)\n",
    "clf = LogisticRegression(class_weight='balanced')\n",
    "###vectorizing data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "x_tfidf=tfidf_vectorizer.fit_transform(x)\n",
    "###stratified k-fold\n",
    "for train_index, test_index in skf.split(x_raw_train, y_raw_train):\n",
    "    x_train, x_test = x_raw_train[train_index], x_raw_train[test_index]\n",
    "    y_train, y_test = y_raw_train[train_index], y_raw_train[test_index]\n",
    "\n",
    "    tfidf_train=tfidf_vectorizer.transform(x_train) \n",
    "    tfidf_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "    \n",
    "    clf.fit(tfidf_train, y_train)\n",
    "    y_pred = clf.predict(tfidf_test)\n",
    "    acc_score += metrics.accuracy_score(y_test, y_pred)\n",
    "#    print(acc_score)\n",
    "    fmeasure+=metrics.f1_score(y_test, y_pred,average='macro')\n",
    "#    print(fmeasure)\n",
    "    pre_score+=metrics.precision_score(y_test, y_pred,average='macro')\n",
    "#    print(pre_score)\n",
    "    rcall_score+=metrics.recall_score(y_test, y_pred,average='macro')\n",
    "#    print(rcall_score)\n",
    "performance_measure_model1.append(pre_score/fold_size)\n",
    "performance_measure_model1.append(rcall_score/fold_size)\n",
    "performance_measure_model1.append(fmeasure/fold_size)\n",
    "performance_measure_model1.append(acc_score/fold_size)\n",
    "print('Train measurment')\n",
    "print('precision','recall','Fmeasure','accuracy')\n",
    "print(performance_measure_model1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_test = tfidf_vectorizer.transform(x_raw_test)\n",
    "\n",
    "test_test_pred = clf.predict(tfidf_test)\n",
    "acc_score = metrics.accuracy_score(y_raw_test, test_test_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y_raw_test, test_test_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y_raw_test, test_test_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y_raw_test, test_test_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54794921875\n",
      "0.46149842450823847\n",
      "0.7271079379323123\n",
      "0.4797677668250742\n"
     ]
    }
   ],
   "source": [
    "df_model2_train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "df_model2_train.shape\n",
    "y_model2_train= df_model2_train[1]\n",
    "x_model2_train=df_model2_train[2]\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.transform(x_model2_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_model2_train)\n",
    "clf2=MultinomialNB()\n",
    "clf2.fit(tfidf_train, y_model2_train)\n",
    "\n",
    "y_pred = clf2.predict(tfidf_test)\n",
    "train_model2_pred = clf2.predict(tfidf_test)\n",
    "acc_score = metrics.accuracy_score(y_model2_train, train_model2_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y_model2_train, train_model2_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y_model2_train, train_model2_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y_model2_train, train_model2_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of valid.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_model2_valid = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "df_model2_valid.shape\n",
    "df_model2_valid.head()\n",
    "y_model2_valid = df_model2_valid[1]\n",
    "x_model2_valid=df_model2_valid[2]\n",
    "\n",
    "tfidf_valid = tfidf_vectorizer.transform(x_model2_valid)\n",
    "\n",
    "valid_model2_pred = clf2.predict(tfidf_valid)\n",
    "acc_score = metrics.accuracy_score(y_model2_valid, valid_model2_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y_model2_valid, valid_model2_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y_model2_valid, valid_model2_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y_model2_valid, valid_model2_pred,average='macro')\n",
    "print(rcall_score)\n",
    "\n",
    "clf2.fit(tfidf_valid, y_model2_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_model2_test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "df_model2_test.shape\n",
    "df_model2_test.head()\n",
    "y_model2_test = df_model2_test[1]\n",
    "x_model2_test=df_model2_test[2]\n",
    "tfidf_test_model2 = tfidf_vectorizer.transform(x_model2_test)\n",
    "\n",
    "test_model2_pred = clf2.predict(tfidf_test_model2)\n",
    "acc_score = metrics.accuracy_score(y_model2_test, test_model2_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y_model2_test, test_model2_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y_model2_test, test_model2_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y_model2_test, test_model2_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mpde a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf_pred = clf.predict(tfidf_test_model2)\n",
    "acc_score = metrics.accuracy_score(y_model2_test, clf_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y_model2_test, clf_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y_model2_test, clf_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y_model2_test, clf_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf2_pred = clf2.predict(x_tfidf)\n",
    "acc_score = metrics.accuracy_score(y, clf2_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(y, clf2_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(y, clf2_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(y, clf2_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing datasets\n",
      "ds1= fake_or_real_news.csv\n",
      "ds2= train.tsv\n",
      "-- fake news\n",
      "Index(['y', 'claim'], dtype='object')\n",
      "3171\n",
      "3164\n",
      "6335\n",
      "-- liar liar\n",
      "Index(['y', 'claim'], dtype='object')\n",
      "{'true', 'false', 'barely-true', 'mostly-true', 'pants-fire', 'half-true'} 10240\n",
      "1676\n",
      "1995\n",
      "{'true', 'false'} 3671\n",
      "false    5159\n",
      "true     4847\n",
      "Name: y, dtype: int64\n",
      "done\n",
      "7004\n",
      "3002\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "ds1 = 'fake_or_real_news.csv'\n",
    "ds2 = 'train.tsv'\n",
    "\n",
    "def get_dataset3_split(dataset1_in, dataset2_in):\n",
    "    try:\n",
    "        print('processing datasets')\n",
    "        print('ds1=', dataset1_in)\n",
    "        print('ds2=', dataset2_in)\n",
    "\n",
    "        print('-- fake news')\n",
    "        df1 = pd.read_csv(dataset1_in, sep=',', usecols=['title','text','label'])\n",
    "        df1['claim'] = df1[['title', 'text']].apply(lambda x: '. '.join(x), axis=1)\n",
    "        del df1['title']\n",
    "        del df1['text']\n",
    "        df1.rename(index=str, columns={'label': 'y'}, inplace=True)\n",
    "        print(df1.keys())\n",
    "        print(len(df1[df1['y']=='REAL']))\n",
    "        print(len(df1[df1['y']=='FAKE']))\n",
    "        df1['y'] = np.where(df1['y'] == 'FAKE', 'false', 'true')\n",
    "        print(len(df1))\n",
    "\n",
    "        print('-- liar liar')\n",
    "        df2 = pd.read_csv(dataset2_in, sep='\\t', header=None, usecols=[1,2], names=['y', 'claim'])\n",
    "        print(df2.keys())\n",
    "        print(set(df2.y), len(df2))\n",
    "        print(len(df2[df2['y'] == 'true']))\n",
    "        print(len(df2[df2['y'] == 'false']))\n",
    "        df2=df2[(df2['y'] == 'true') | (df2['y'] == 'false')]\n",
    "        print(set(df2.y), len(df2))\n",
    "\n",
    "        df3=pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        print(df3['y'].value_counts())\n",
    "        print('done')\n",
    "        return train_test_split(df3['claim'], df3['y'], test_size=0.30, random_state=35)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "ds3_train, ds3_test, ds3_y_train, ds3_y_test = get_dataset3_split(ds1,ds2)\n",
    "print(len(ds3_train))\n",
    "print(len(ds3_test))\n",
    "#print(ds3_y_train.get_values()[1], ds3_train.get_values()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuement of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train measurment\n",
      "precision recall Fmeasure accuracy\n",
      "[0.7627188147918379, 0.6968113122568707, 0.6800395948141202, 0.7020228827206745]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=.01)\n",
    "acc_score,fmeasure,pre_score,rcall_score=0,0,0,0\n",
    "performance_measure_model3=[]\n",
    "for train_index, test_index in skf.split(ds3_train, ds3_y_train):\n",
    "    x_train, x_test = ds3_train.iloc[train_index], ds3_train.iloc[test_index]\n",
    "    y_train, y_test = ds3_y_train.iloc[train_index], ds3_y_train.iloc[test_index]\n",
    "    \n",
    "    tfidf_train=tfidf_vectorizer.transform(x_train.values.astype('U')) \n",
    "    tfidf_test = tfidf_vectorizer.transform(x_test.values.astype('U'))\n",
    "    clf.fit(tfidf_train, y_train)\n",
    "    y_pred = clf.predict(tfidf_test)\n",
    "    acc_score += metrics.accuracy_score(y_test, y_pred)\n",
    "#    print(acc_score)\n",
    "    fmeasure+=metrics.f1_score(y_test, y_pred,average='macro')\n",
    "#    print(fmeasure)\n",
    "    pre_score+=metrics.precision_score(y_test, y_pred,average='macro')\n",
    "#    print(pre_score)\n",
    "    rcall_score+=metrics.recall_score(y_test, y_pred,average='macro')\n",
    "#    print(rcall_score)\n",
    "performance_measure_model3.append(pre_score/fold_size)\n",
    "performance_measure_model3.append(rcall_score/fold_size)\n",
    "performance_measure_model3.append(fmeasure/fold_size)\n",
    "performance_measure_model3.append(acc_score/fold_size)\n",
    "print('Train measurment')\n",
    "print('precision','recall','Fmeasure','accuracy')\n",
    "print(performance_measure_model3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105263157894737\n",
      "0.6845801005793597\n",
      "0.7687326869806095\n",
      "0.6969378665231403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_test = tfidf_vectorizer.transform(ds3_test)\n",
    "\n",
    "test_test_pred = clf.predict(tfidf_test)\n",
    "acc_score = metrics.accuracy_score(ds3_y_test, test_test_pred)\n",
    "print(acc_score)\n",
    "fmeasure=metrics.f1_score(ds3_y_test, test_test_pred,average='macro')\n",
    "print(fmeasure)\n",
    "pre_score=metrics.precision_score(ds3_y_test, test_test_pred,average='macro')\n",
    "print(pre_score)\n",
    "rcall_score=metrics.recall_score(ds3_y_test, test_test_pred,average='macro')\n",
    "print(rcall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
