{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POOJA\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
      "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
      "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
      "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
      "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive\n",
      "5  RT @GregAbbott_TX: @TedCruz: \"On my first day ...  Positive\n",
      "6  RT @warriorwoman91: I liked her and was happy ...  Negative\n",
      "7  Going on #MSNBC Live with @ThomasARoberts arou...   Neutral\n",
      "8  Deer in the headlights RT @lizzwinstead: Ben C...  Negative\n",
      "9  RT @NancyOsborne180: Last night's debate prove...  Negative\n",
      "                                                 text sentiment\n",
      "1     scottwalker didnt catch the full gopdebate l...  Positive\n",
      "3     robgeorge that carly fiorina is trending  ho...  Positive\n",
      "4     danscavino gopdebate w realdonaldtrump deliv...  Positive\n",
      "5     gregabbott_tx tedcruz on my first day i will...  Positive\n",
      "6     warriorwoman91 i liked her and was happy whe...  Negative\n",
      "8   deer in the headlights   lizzwinstead ben cars...  Negative\n",
      "9     nancyosborne180 last nights debate proved it...  Negative\n",
      "10  jgreendc realdonaldtrump in all fairness billc...  Negative\n",
      "11    waynedupreeshow just woke up to tweet this o...  Positive\n",
      "12  me reading my familys comments about how great...  Negative\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 319,194\n",
      "Trainable params: 319,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 13s - loss: 0.4335 - acc: 0.8170\n",
      "Epoch 2/2\n",
      " - 12s - loss: 0.3661 - acc: 0.8467\n",
      "score: 0.38\n",
      "acc: 0.84\n",
      "pos_acc 46.875 %\n",
      "neg_acc 93.93564356435643 %\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "[0.8602308  0.13976923]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 100)           38500     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2402      \n",
      "=================================================================\n",
      "Total params: 104,902\n",
      "Trainable params: 104,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 2s - loss: 0.4199 - acc: 0.8206\n",
      "Epoch 2/2\n",
      " - 2s - loss: 0.3211 - acc: 0.8634\n",
      "score: 0.37\n",
      "acc: 0.84\n",
      "pos_acc 47.91666666666667 %\n",
      "neg_acc 94.55445544554455 %\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "[0.9565835  0.04341643]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "import re\n",
    "\n",
    "# parameters\n",
    "max_fatures = 500\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "dropout = 0.1\n",
    "dropout_1d = 0.4\n",
    "recurrent_dropout = 0.1\n",
    "random_state = 1324\n",
    "validation_size = 1000\n",
    "batch_size = 16\n",
    "epochs=2\n",
    "verbose= 2\n",
    "\n",
    "# Preprocess and Read Data \n",
    "df = pd.read_csv('dataset_sentiment.csv')\n",
    "df = df[['text','sentiment']]\n",
    "print(df[0:10])\n",
    "\n",
    "df = df[df.sentiment != \"Neutral\"] \n",
    "df['text'] = df['text'].apply(lambda x: x.lower()) #\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('rt',' '))\n",
    "df['text'] = df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "print(df[0:10])    \n",
    "    \n",
    "tok = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tok.fit_on_texts(df['text'].values)\n",
    "X = tok.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Model1:Using LSTM\n",
    "def model_1():\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "    nn.add(SpatialDropout1D(dropout_1d))\n",
    "    nn.add(LSTM(lstm_out, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    nn.add(Dense(2, activation='softmax'))\n",
    "    nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(nn.summary())\n",
    "    return nn\n",
    "\n",
    "#Model2:Using ConvNet\n",
    "def model_2():\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "    nn.add(Convolution1D(filters=100,kernel_size=3, padding=\"valid\", activation=\"relu\", strides=1))\n",
    "    nn.add(MaxPooling1D(pool_size=2))\n",
    "    nn.add(Flatten())\n",
    "    nn.add(Dense(2, activation='softmax'))\n",
    "    nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(nn.summary())\n",
    "    return nn\n",
    "\n",
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "\n",
    "#Split Dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\n",
    "\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "\n",
    "#Evaluation Function \n",
    "def evaluation(nn):\n",
    "    \n",
    "    score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (accuracy))\n",
    "\n",
    "    pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "    for x in range(len(X_validate)):\n",
    "        result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "        if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "            if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "            else: pos_ok += 1\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "        else: pos_cnt += 1\n",
    "\n",
    "    print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "    print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n",
    "\n",
    "    X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "    X2 = tok.texts_to_sequences(X2)\n",
    "    X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "    print(X2)\n",
    "    print(nn.predict(X2, batch_size=1, verbose = 2)[0])\n",
    "\n",
    "\n",
    "\n",
    "####Results####\n",
    "\n",
    "nn_1=model_1()\n",
    "nn_1.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n",
    "evaluation(nn_1)\n",
    "\n",
    "\n",
    "nn_2=model_2()\n",
    "nn_2.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n",
    "evaluation(nn_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating MetaData\n",
    "\n",
    "from rdflib import Namespace, Graph, Literal\n",
    "from rdflib.namespace import FOAF, OWL, XSD, RDFS, DCTERMS, DOAP, DC, RDF\n",
    "\n",
    "\n",
    "prov = Namespace('http://www.w3.org/ns/prov#')\n",
    "dcat = Namespace('http://www.w3.org/ns/dcat#')\n",
    "mexalgo = Namespace('http://mex.aksw.org/mex-algo#')\n",
    "mexperf = Namespace('http://mex.aksw.org/mex-perf#')\n",
    "mexcore = Namespace('http://mex.aksw.org/mex-core#')\n",
    "this = Namespace('http://mex.aksw.org/examples/')\n",
    "\n",
    "g = Graph()\n",
    "# Create Binding\n",
    "g.bind('dct',DCTERMS)\n",
    "g.bind('owl',OWL)\n",
    "g.bind('foaf',FOAF)\n",
    "g.bind('xsd', XSD)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('doap', DOAP)\n",
    "g.bind('dc', DC)\n",
    "g.bind('prov', prov)\n",
    "g.bind('dcat', dcat)\n",
    "g.bind('mexalgo',mexalgo)\n",
    "g.bind('mexperf',mexperf)\n",
    "g.bind('mexcore',mexcore)\n",
    "g.bind('this',this)\n",
    "\n",
    "\n",
    "g.add((this.pooja_task3,mexcore.Experiment, prov.Entity))\n",
    "g.add((this.pooja_task3,mexcore.ApplicationContext, prov.Entity))\n",
    "g.add((this.pooja_task3,DCTERMS.date, Literal('2018-07-22',datatype=XSD.date)))\n",
    "g.add((this.pooja_task3,FOAF.givenName, Literal('Pooja Bhatia')))\n",
    "g.add((this.pooja_task3,FOAF.mbox, Literal('pooja12.3.92@gmail.com')))\n",
    "\n",
    "\n",
    "#Configuration of Model 1\n",
    "g.add((this.configuration1,RDF.type,mexcore.ExperimentConfiguration))\n",
    "g.add((this.configuration1,prov.used, this.model1))\n",
    "g.add((this.configuration1,prov.wasStartedBy,this.pooja_task3))\n",
    "\n",
    "#Configuration of Model 2\n",
    "g.add((this.configuration2,RDF.type,mexcore.ExperimentConfiguration))\n",
    "g.add((this.configuration2,prov.used, this.model2))\n",
    "g.add((this.configuration2,prov.wasStartedBy,this.pooja_task3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g.add((this.hyerparameter_model1,mexalgo.HyperParameterCollection,prov.Entity))\n",
    "g.add((this.hyerparameter1,RDFS.label,Literal('HyperParameterCollection')))\n",
    "g.add((this.hyerparameter_model1,prov.hadMember,this.hyerparameter1))\n",
    "\n",
    "g.add((this.hyerparameter_model2,mexalgo.HyperParameterCollection,prov.Entity))\n",
    "g.add((this.hyerparameter2,RDFS.label,Literal('HyperParameterCollection')))\n",
    "g.add((this.hyerparameter_model2,prov.hadMember,this.hyerparameter2))\n",
    "\n",
    "\n",
    "g.add((this.hyerparameter1,mexalgo.HyperParameter,prov.Entity))\n",
    "g.add((this.hyerparameter1,RDFS.label, Literal('LSTM')))\n",
    "g.add((this.hyerparameter1,DCTERMS.identifier, Literal('LSTM')))\n",
    "g.add((this.hyerparameter1,prov.value, Literal('196',datatype=XSD.float)))\n",
    "\n",
    "\n",
    "g.add((this.hyerparameter2,mexalgo.HyperParameter,prov.Entity))\n",
    "g.add((this.hyerparameter2,RDFS.label, Literal('ConvNet')))\n",
    "g.add((this.hyerparameter2,DCTERMS.identifier, Literal('ConvNet')))\n",
    "g.add((this.hyerparameter2,prov.value, Literal('100',datatype=XSD.float)))\n",
    "\n",
    "\n",
    "g.add((this.execution1,mexcore.ExecutionOverall,prov.Entity))\n",
    "g.add((this.execution1,prov.generated,this.performance_measures1))\n",
    "g.add((this.execution1,prov.used,this.test))\n",
    "g.add((this.execution1,prov.used,this.hyerparameter_model1))\n",
    "g.add((this.execution1,prov.used,this.model1))\n",
    "\n",
    "g.add((this.performance_measures1,mexcore.PerformanceMeasure,prov.Entity))\n",
    "g.add((this.performance_measures1,mexperf.score,Literal('0.38',datatype=XSD.float)))\n",
    "g.add((this.performance_measures1,mexperf.accuracy,Literal('0.84',datatype=XSD.float)))\n",
    "g.add((this.performance_measures1,prov.wasGeneratedBy,this.execution1))\n",
    "\n",
    "\n",
    "g.add((this.execution2,mexcore.ExecutionOverall,prov.Entity))\n",
    "g.add((this.execution2,prov.generated,this.performance_measures2))\n",
    "g.add((this.execution2,prov.used,this.test))\n",
    "g.add((this.execution2,prov.used,this.model2))\n",
    "\n",
    "g.add((this.performance_measures2,mexcore.PerformanceMeasure,prov.Entity))\n",
    "g.add((this.performance_measures2,mexperf.score,Literal('0.38',datatype=XSD.float)))\n",
    "g.add((this.performance_measures2,mexperf.accuracy,Literal('0.85',datatype=XSD.float)))\n",
    "g.add((this.performance_measures2,prov.wasGeneratedBy,this.execution2))\n",
    "\n",
    "\n",
    "g.add((this.model1,mexalgo.Algorithm,prov.Entity))\n",
    "g.add((this.model1,RDFS.label,Literal('LSTM')))\n",
    "g.add((this.model1,mexalgo.hasHyperParameter,this.hyerparameter1))\n",
    "\n",
    "g.add((this.model2,mexalgo.Algorithm,prov.Entity))\n",
    "g.add((this.model2,RDFS.label,Literal('ConvNet')))\n",
    "g.add((this.model2,mexalgo.hasHyperParameter,this.hyerparameter2))\n",
    "\n",
    "\n",
    "with open('pooja_Exer3_metadata.ttl','wb') as f:\n",
    "    f.write(g.serialize(format='turtle'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
