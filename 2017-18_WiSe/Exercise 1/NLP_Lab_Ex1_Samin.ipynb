{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\samin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk import DefaultTagger as df\n",
    "from nltk import UnigramTagger as ut\n",
    "from nltk import BigramTagger as bt\n",
    "from nltk import TrigramTagger as tg\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('treebank') \n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "def features(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1]\n",
    "    }\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def training (training_sentences):\n",
    "\n",
    "            #this function considers each sentences in the training set, then saves the features of each word in each        //sentences in X,\n",
    "    #and saves the tag of all words in all sentences into y. So, we have the features of all words appeard in training //set in x and all tags in y.\n",
    "    X, y = transform_to_dataset(training_sentences)\n",
    "    size=100\n",
    "    return clf.fit(X[:size], y[:size])\n",
    "\n",
    "    \n",
    "    #print(list(pos_tag(word_tokenize('Hello world, lets do something awesome today!'))))\n",
    "\n",
    "\n",
    "def pos_tag(sentence, classifier):\n",
    "    print('checking...')\n",
    "    tagged_sentence = []\n",
    "    tags = classifier.predict([features(sentence, index) for index in range(len(sentence))])    \n",
    "    return zip(sentence, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14325887835449141\n",
      "0.29989554877068936\n",
      "0.006990197653864695\n",
      "0.005865338261288768\n",
      "0.2421259842519685\n"
     ]
    }
   ],
   "source": [
    "x1 = treebank\n",
    "x2 = brown \n",
    "#x3 = farsi\n",
    "\n",
    "size = 100\n",
    "patterns = [(r'.*ing$', 'VBG'), (r'.*ed$', 'VBD'), (r'.*es$', 'VBZ'), (r'.*ould$', 'MD'), (r'.*\\'s$', 'NN$'),               \n",
    "             (r'.*s$', 'NNS'), (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')]\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "annotated_sent1 = x1.tagged_sents()\n",
    "cutoff1 = int(.75 * len(annotated_sent1)) #devide part of the corpus as training set and another part as test set\n",
    "training_sentences1 = annotated_sent1[:cutoff1] \n",
    "test_sentences1 = annotated_sent1[cutoff1:]\n",
    "X1,y1 = transform_to_dataset(test_sentences1)\n",
    "\n",
    "model1 = training(training_sentences1) #clf_eng_treebank\n",
    "model31 = nltk.DefaultTagger('NN')\n",
    "model32 = nltk.UnigramTagger(train_sents)\n",
    "model33 = nltk.BigramTagger(train_sents)\n",
    "model34 = nltk.TrigramTagger(train_sents)\n",
    "model35 = nltk.RegexpTagger(patterns)\n",
    "performance11 = model1.score (X1,y1)\n",
    "performance131 = model31.evaluate(test_sentences1)\n",
    "performance132 = model32.evaluate(test_sentences1)\n",
    "performance133 = model33.evaluate(test_sentences1)\n",
    "performance134 = model34.evaluate(test_sentences1)\n",
    "performance135 = model35.evaluate(test_sentences1)\n",
    "print (performance131)\n",
    "print (performance132)\n",
    "print (performance133)\n",
    "print (performance134)\n",
    "print (performance135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
