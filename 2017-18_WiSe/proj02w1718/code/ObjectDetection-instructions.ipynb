{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.\tPre-requirments: Python 3.6 and python 2.7\n",
    "2.\tInstall Tensorflow library using pip3\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Script is used by me to turn my train and test images important info into an xml file. xmin, ymin, xmax and ymax are \n",
    "#showing the coordination od bounding box around an object. as in my dataset the whole image was the object, I putted xmin and\n",
    "#ymin equal to zero and xmax and ymax equal to the height and width of image.\n",
    "#in case your dataset contains photos which objects are only a part of it, use labelimg application to draw bounding boxex,\n",
    "#then an xml file is saved by labelimg automatically\n",
    "#filename: script.py\n",
    "from PIL import Image\n",
    "import os\n",
    "from xml.etree.ElementTree import parse, Element\n",
    "folder = 'C:/Users/D071941/Documents/NLP/test'\n",
    "save_path = 'C:/Users/D071941/Documents/NLP/testannotations'\n",
    "for file in os.listdir(folder):\n",
    "    filename = os.fsdecode(file)\n",
    "    filepath = os.path.join(folder,filename)\n",
    "    im = Image.open(filepath)\n",
    "    width, height = im.size\n",
    "    newfilename = filename.replace('.jpg','.xml')\n",
    "    newfilepath = os.path.join(save_path, newfilename)\n",
    "    tree = parse('C:/Users/D071941/Documents/NLP/xmlformat.xml')\n",
    "    root = tree.getroot()\n",
    "    tree.find('.//filename').text = filename\n",
    "    tree.find('.//path').text = filepath\n",
    "    tree.find('.//width').text = str(width)\n",
    "    tree.find('.//height').text = str(height)\n",
    "    tree.find('.//xmax').text = str(width)\n",
    "    tree.find('.//ymax').text = str(height)\n",
    "    if filename [6]=='0':\n",
    "        tree.find('.//name').text = 'person'\n",
    "    elif filename [6]=='1':\n",
    "        tree.find('.//name').text = 'forest'\n",
    "    elif filename [6]=='2':\n",
    "        tree.find('.//name').text = 'highway'\n",
    "    elif filename [6]=='4':\n",
    "        tree.find('.//name').text = 'mountain'\n",
    "    elif filename [6]=='5':\n",
    "        tree.find('.//name').text = 'building'\n",
    "    file_obj = open(newfilepath,\"w\") \n",
    "    tree.write(newfilepath, xml_declaration=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the \"xmlformat.xml\" used in previous code\n",
    "<annotation>\n",
    "\t<folder>train</folder>\n",
    "\t<filename>image_0005.jpg</filename>\n",
    "\t<path>C:\\Users\\D071941\\Documents\\NLP\\Faces_easy\\image_0005.jpg</path>\n",
    "\t<source>\n",
    "\t\t<database>Unknown</database>\n",
    "\t</source>\n",
    "\t<size>\n",
    "\t\t<width>290</width>\n",
    "\t\t<height>330</height>\n",
    "\t\t<depth>3</depth>\n",
    "\t</size>\n",
    "\t<segmented>0</segmented>\n",
    "\t<object>\n",
    "\t\t<name>building</name>\n",
    "\t\t<pose>Unspecified</pose>\n",
    "\t\t<truncated>0</truncated>\n",
    "\t\t<difficult>0</difficult>\n",
    "\t\t<bndbox>\n",
    "\t\t\t<xmin>0</xmin>\n",
    "\t\t\t<ymin>0</ymin>\n",
    "\t\t\t<xmax>284</xmax>\n",
    "\t\t\t<ymax>325</ymax>\n",
    "\t\t</bndbox>\n",
    "\t</object>\n",
    "</annotation>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this file is called \"xml2csvscript.py\", it changes the xml file generated in previous step to CSV file.\n",
    "#ofcourse you could only generate the csv from the start.\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text))\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "def main():\n",
    "    for i in ['C:/Users/D071941/Documents/NLP/testannotations']:\n",
    "        folder = os.path.basename(os.path.normpath(i))\n",
    "        xml_df = xml_to_csv(i)\n",
    "        xml_df.to_csv(folder+'/test.csv', index=None)\n",
    "        print('Successfully converted xml to csv.')\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn .csv files into TF records\n",
    "#you need path to images and path to csv files\n",
    "\"\"\"\n",
    "Usage:\n",
    "  # From tensorflow/models/\n",
    "  # Create train data:\n",
    "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\n",
    "  # Create test data:\n",
    "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('csv_input', 'data/test.csv', '')\n",
    "flags.DEFINE_string('output_path', 'data/test.record', '')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "# TO-DO replace this with label map\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'person':\n",
    "        return 1\n",
    "    elif row_label == 'forest':\n",
    "        return 2\n",
    "    elif row_label == 'highway':\n",
    "        return 3\n",
    "    elif row_label == 'mountain':\n",
    "\t    return 4\n",
    "    elif row_label == 'building':\n",
    "\t    return 5\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    print(\"file name:{}\", group.filename)\n",
    "    print(\"width:{}\\theight:{}\", width, height)\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        print(\"xmin:{}\", row['xmin'])\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    path = os.path.join(os.getcwd(), 'test')\n",
    "    examples = pd.read_csv(FLAGS.csv_input)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now you have your dataset ready, you  could start training\n",
    "\n",
    "3.\tFind the directory of your python 3.6 and go to this path “Python36\\Lib\\site-packages\\tensorflow”\n",
    "4.\tRetrieve the object detection API by running: \n",
    "5.\t“git clone https://github.com/tensorflow/models”\n",
    "6.\tInstall required libraries mentioned here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md\n",
    "7.\tNow if you want to run on cloud go to this path in your computer:\n",
    "“Python36\\Lib\\site-packages\\tensorflow\\models\\research” \n",
    "And make the following changes in the required files:\n",
    "•\tReplace setup.py with this script:\n",
    "\"\"\"\n",
    "\"\"\"Setup script for object_detection.\"\"\"\n",
    "\n",
    "import logging\n",
    "import subprocess\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "from setuptools.command.install import install\n",
    "\n",
    "class CustomCommands(install):\n",
    "\n",
    "\tdef RunCustomCommand(self, command_list):\n",
    "\t\tp = subprocess.Popen(\n",
    "        command_list,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "\t\tstdout_data, _ = p.communicate()\n",
    "\t\tlogging.info('Log command output: %s', stdout_data)\n",
    "\t\tif p.returncode != 0:\n",
    "\t\t\traise RuntimeError('Command %s failed: exit code: %s' %\n",
    "                         (command_list, p.returncode))\n",
    "\n",
    "\tdef run(self):\n",
    "\t\tself.RunCustomCommand(['apt-get', 'update'])\n",
    "\t\tself.RunCustomCommand(\n",
    "          ['apt-get', 'install', '-y', 'python-tk'])\n",
    "\t\tinstall.run(self)\n",
    "\n",
    "REQUIRED_PACKAGES = ['Pillow>=1.0', 'protobuf>=3.3.0', 'Matplotlib>=2.1']\n",
    "\n",
    "setup(\n",
    "    name='object_detection',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    include_package_data=True,\n",
    "    packages=[p for p in find_packages() if p.startswith('object_detection')],\n",
    "    description='Tensorflow Object Detection Library',\n",
    " cmdclass={\n",
    "        'install': CustomCommands,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "•\tIn object_detection directory, line 184 of evaluator.py, change\n",
    "\t\ttf.train.get_or_create_global_step()\n",
    "to\n",
    "\t\ttf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "•\tIn “samples\\cloud” change the runtime version to 1.2, at the time I am writing this guide version 1.4 is released but it has some defects which leads to errors when you run Faster R-CNN and RFCN models.\n",
    "•\tIn object_detection/utils/visualization_utils.py, line 24 (before import matplotlib.pyplot as plt) add:\n",
    "\t\timport matplotlib\n",
    "\t\tmatplotlib.use('agg')\n",
    "\n",
    "•\tFinally, in line 103 of object_detection/builders/optimizer_builder.py, change\n",
    "\t\t tf.train.get_or_create_global_step()\n",
    "to\n",
    "tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. From tensorflow/models/research/ run:\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "#9.You should add these paths to your system environment, So first replace the true path to your python36 in them and then\n",
    "#Easily, add a new system variable called PYTHONPATH, add these paths to it\n",
    "#go to the “path” in your system variables and add “%PYTHONPATH%” to it.\n",
    "“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models”\n",
    "“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research”\n",
    "“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research\\slim”\n",
    "“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection”\n",
    "\n",
    "#10.\tTest the installations: \n",
    "python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "11.Set up the GCP project and bucket (name it something memorable) — and it should create ‘your first project’. \n",
    "You should do all the things mentioned in this link: https://cloud.google.com/solutions/creating-object-detection-application-tensorflow step by step. \n",
    "Then in your google cloud platform environment, from the storage in left hand nav bar, \n",
    "you could make a new bucket with a unique name. \n",
    "12.Enable Machine Learning (ML) Engine API’s — should be in the left hand nav bar of your project\n",
    "13.nstall the Google Cloud SDK on your local machine, and do the authentication and choose your project\n",
    "by the id of project you made in step 11.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "14. if you want to train locally, you should add a new folder to “C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection”\n",
    "called mytraining for example. in this folder, you have: 1. TFrecord of train and test data 2. config file of the model\n",
    "you want to train like faster_rcnn_inception_v2_coco.config --> obtain config files from this path:\n",
    "\n",
    "C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection\\samples\\configs\n",
    "\n",
    "3.you should have the initial checkpoints for the model to start. To obtain the checkpoints, open this file:\n",
    "\n",
    "C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection\\g3doc\\detection_model_zoo.md\n",
    "\n",
    "Download the zip file corresponding to your desired model from the links, you can file 3 check point file there. 4. you need a label map,\n",
    "the label map, maps each class name to an int\n",
    "add all of them to mytraining directory.\n",
    "if you want to run on google cloud, add all these files to your bucket.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample config file mentioned in step 14\n",
    "#rfcn_resnet101_coco.config\n",
    "# R-FCN with Resnet-101 (v1),  configuration for MSCOCO Dataset.\n",
    "# Users should configure the fine_tune_checkpoint field in the train config as\n",
    "# well as the label_map_path and input_path fields in the train_input_reader and\n",
    "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
    "# should be configured.\n",
    "#example of path on google cloud: \"gs://object_detection_bucket_samin/data/train.record\"\n",
    "\n",
    "model {\n",
    "  faster_rcnn {\n",
    "    num_classes: 5\n",
    "    image_resizer {\n",
    "      keep_aspect_ratio_resizer {\n",
    "        min_dimension: 600\n",
    "        max_dimension: 1024\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: 'faster_rcnn_resnet101'\n",
    "      first_stage_features_stride: 16\n",
    "    }\n",
    "    first_stage_anchor_generator {\n",
    "      grid_anchor_generator {\n",
    "        scales: [0.25, 0.5, 1.0, 2.0]\n",
    "        aspect_ratios: [0.5, 1.0, 2.0]\n",
    "        height_stride: 16\n",
    "        width_stride: 16\n",
    "      }\n",
    "    }\n",
    "    first_stage_box_predictor_conv_hyperparams {\n",
    "      op: CONV\n",
    "      regularizer {\n",
    "        l2_regularizer {\n",
    "          weight: 0.0\n",
    "        }\n",
    "      }\n",
    "      initializer {\n",
    "        truncated_normal_initializer {\n",
    "          stddev: 0.01\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    first_stage_nms_score_threshold: 0.0\n",
    "    first_stage_nms_iou_threshold: 0.7\n",
    "    first_stage_max_proposals: 300\n",
    "    first_stage_localization_loss_weight: 2.0\n",
    "    first_stage_objectness_loss_weight: 1.0\n",
    "    second_stage_box_predictor {\n",
    "      rfcn_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          op: CONV\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.0\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              stddev: 0.01\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        crop_height: 18\n",
    "        crop_width: 18\n",
    "        num_spatial_bins_height: 3\n",
    "        num_spatial_bins_width: 3\n",
    "      }\n",
    "    }\n",
    "    second_stage_post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 0.0\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 300\n",
    "      }\n",
    "      score_converter: SOFTMAX\n",
    "    }\n",
    "    second_stage_localization_loss_weight: 2.0\n",
    "    second_stage_classification_loss_weight: 1.0\n",
    "  }\n",
    "}\n",
    "\n",
    "train_config: {\n",
    "  batch_size: 1\n",
    "  optimizer {\n",
    "    momentum_optimizer: {\n",
    "      learning_rate: {\n",
    "        manual_step_learning_rate {\n",
    "          initial_learning_rate: 0.0003\n",
    "          schedule {\n",
    "            step: 0\n",
    "            learning_rate: .0003\n",
    "          }\n",
    "          schedule {\n",
    "            step: 900000\n",
    "            learning_rate: .00003\n",
    "          }\n",
    "          schedule {\n",
    "            step: 1200000\n",
    "            learning_rate: .000003\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  gradient_clipping_by_norm: 10.0\n",
    "  fine_tune_checkpoint: \"$PATH_TO_BE_CONFIGURED/model.ckpt\"\n",
    "  from_detection_checkpoint: true\n",
    "  # Note: The below line limits the training process to 200K steps, which we\n",
    "  # empirically found to be sufficient enough to train the pets dataset. This\n",
    "  # effectively bypasses the learning rate schedule (the learning rate will\n",
    "  # never decay). Remove the below line to train indefinitely.\n",
    "  num_steps: 100000\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path:  \"$PATH_TO_BE_CONFIGURED/train.record\"\n",
    "  }\n",
    "    \n",
    "  label_map_path: \"$PATH_TO_BE_CONFIGURED/label_map.pbtxt\"\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 8000\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  \n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path:  \"$PATH_TO_BE_CONFIGURED/test.record\"\n",
    "  }\n",
    "  label_map_path: \"$PATH_TO_BE_CONFIGURED/label_map.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "  num_epochs: 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample label map as mentioned in 14\n",
    "#label_map.pbtxt\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'person'\n",
    "}\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'forest'\n",
    "}\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'highway'\n",
    "}\n",
    "item {\n",
    "  id: 4\n",
    "  name: 'mountain'\n",
    "}\n",
    "item {\n",
    "  id: 5\n",
    "  name: 'building'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "15. add export_inference_graph.py and train.py from the path \"Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection\"\n",
    "to mytraining if you want to train locally.\n",
    "export_inference_graph.py is used to turn the final trained checkpoint into a frozen interface graph which cpould be used for\n",
    "testing\n",
    "16.Package the object detection config files on you local machine from the path:\n",
    "“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research”\n",
    "By running:\n",
    "python setup.py sdist\n",
    "cd slim\n",
    "python setup.py sdist\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train locally:\n",
    "#go to “C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research”\n",
    "python object_detection/train.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=${PATH_TO_YOUR_PIPELINE_CONFIG} \\ #path to *.config I mentioned before\n",
    "    --train_dir=${PATH_TO_TRAIN_DIR} #name a new folder which will contain your trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.To train a model on cloud, go to :\n",
    "#“C:\\Program Files\\Python36\\Lib\\site-packages\\tensorflow\\models\\research”\n",
    "#Open git bash, run the following commands, but you should change them according to your paths:\n",
    "export CLOUDSDK_PYTHON=\"C:\\Python27\\python.exe\" \n",
    "\n",
    "gcloud ml-engine jobs submit training `whoami`_object_detection_eval_`date +%s`     \n",
    "--job-dir= gs://object_detection_bucket_samin /train \\ #same as train_dir\n",
    "--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \\    \n",
    "--module-name object_detection.train  \\\n",
    "--region us-central1  \\\n",
    " --config object_detection/samples/cloud/cloud.yml \\     \n",
    "--    \n",
    " --train_dir=gs:// object_detection_bucket_samin /train   \\ #new folder for training jobs\n",
    "--pipeline_config_path=gs://object_detection_bucket_samin/data/rfcn_resnet101_coco.config  \\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to observe the plots and histograms of training jobs you should open the google cloud shell\n",
    "from the main nav of your google cloud platform (4'th icon from right side):\n",
    "And run: \"\"\"\n",
    "tensorboard --logdir=gs://${GCP_BUCKET_NAME} --port=8080\n",
    "\"\"\"\n",
    "17.After training finished you should turn your desired checkpoint of model into a frozen interface graph to use it for testing on test images. \n",
    "So, go to the path that you have your “export_inference_graph.py” and you should run this command:\"\"\"\n",
    "py -3 export_inference_graph.py --input_type image_tensor --pipeline_config_path ./ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix ./models/ train5_rfcn /model.ckpt-100004 --output_directory ./fine_tuned_model_5classes\n",
    "#Do not forget to change the paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "18.To test your images, you could use a prepared file called “object_detection_tutorial.ipynb” in the path:\n",
    "“Python36\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection”. \n",
    "You should open this file in Jupyter and follow the instructions in commands. \n",
    "You should change some paths to frozen file etc. It’s straight forward. I converted this file to python, I put it bellow\"\"\"\n",
    "#object_detection_tutorial.py\n",
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# # Object Detection Demo\n",
    "# Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start.\n",
    "\n",
    "# # Imports\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "\n",
    "\n",
    "# ## Env setup\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# This is needed to display the images.\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# ## Object detection imports\n",
    "# Here are the imports from the object detection module.\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "# # Model preparation \n",
    "\n",
    "# ## Variables\n",
    "# \n",
    "# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "# \n",
    "# By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "\n",
    "# ## Download Model\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())\n",
    "\n",
    "\n",
    "# ## Load a (frozen) Tensorflow model into memory.\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "# ## Loading label map\n",
    "# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "\n",
    "# ## Helper code\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "# # Detection\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    # Definite input and output Tensors for detection_graph\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    # Each box represents a part of the image where a particular object was detected.\n",
    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "    # Each score represent how level of confidence for each of the objects.\n",
    "    # Score is shown on the result image, together with the class label.\n",
    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "    for image_path in TEST_IMAGE_PATHS:\n",
    "      image = Image.open(image_path)\n",
    "      # the array based representation of the image will be used later in order to prepare the\n",
    "      # result image with boxes and labels on it.\n",
    "      image_np = load_image_into_numpy_array(image)\n",
    "      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "      image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "      # Actual detection.\n",
    "      (boxes, scores, classes, num) = sess.run(\n",
    "          [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "      # Visualization of the results of a detection.\n",
    "      vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          np.squeeze(boxes),\n",
    "          np.squeeze(classes).astype(np.int32),\n",
    "          np.squeeze(scores),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=8)\n",
    "      plt.figure(figsize=IMAGE_SIZE)\n",
    "      plt.imshow(image_np)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
