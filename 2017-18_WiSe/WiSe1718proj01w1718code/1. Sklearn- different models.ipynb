{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download json data from here: http://dbpedia.org/snorql\n",
    "\n",
    "I used the following query for actors (changed \"Actor\" to \"City\",... for the rest): \n",
    "\n",
    "PREFIX dbpedia0: <http://dbpedia.org/ontology/>\n",
    "\n",
    "PREFIX dbpedia2: <http://dbpedia.org/property/>\n",
    "\n",
    "PREFIX dbpedia: <http://dbpedia.org/resource/>\n",
    "\n",
    "select distinct ?name ?abstract where {\n",
    "\n",
    "         ?instance a dbpedia0:Actor.\n",
    "         \n",
    "         ?instance foaf:name ?name.\n",
    "         \n",
    "         ?instance dbpedia0:abstract ?abstract.\n",
    "         \n",
    "         filter(langMatches(lang(?abstract),\"en\"))\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load json files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#from pprint import pprint\n",
    "\n",
    "data_actor = json.load(open('musicalArtist.json'))\n",
    "data_city = json.load(open('city.json'))\n",
    "data_celestialbody = json.load(open('celestialbody.json'))\n",
    "data_education = json.load(open('educationalInstitution.json'))\n",
    "data_lake = json.load(open('lake.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training- and testing dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use Data to create Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person(Actor)\n",
    "\n",
    "person_names = []\n",
    "person_abstracts = []\n",
    "\n",
    "length_p = len(data_actor['results']['bindings'])\n",
    "\n",
    "for i in range(length_p):\n",
    "    name = data_actor['results']['bindings'][i]['name']['value']\n",
    "    abstract = data_actor['results']['bindings'][i]['abstract']['value']\n",
    "    person_names.append(name)\n",
    "    person_abstracts.append(abstract)\n",
    "\n",
    "\n",
    "# City\n",
    "city_name = []\n",
    "city_abstract = []\n",
    "\n",
    "length_c = len(data_city['results']['bindings'])\n",
    "\n",
    "for j in range(length_c):\n",
    "    name = data_city['results']['bindings'][j]['name']['value']\n",
    "    abstract =  data_city['results']['bindings'][j]['abstract']['value']\n",
    "    city_name.append(name)\n",
    "    city_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# CelestialBody\n",
    "cb_name = []\n",
    "cb_abstract = []\n",
    "\n",
    "length_cb = len(data_celestialbody['results']['bindings'])\n",
    "\n",
    "for k in range(length_cb):\n",
    "    name = data_celestialbody['results']['bindings'][k]['name']['value']\n",
    "    abstract =  data_celestialbody['results']['bindings'][k]['abstract']['value']\n",
    "    cb_name.append(name)\n",
    "    cb_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# EducationalInstitution\n",
    "ei_name = []\n",
    "ei_abstract = []\n",
    "\n",
    "length_ei = len(data_education['results']['bindings'])\n",
    "\n",
    "for l in range(length_ei):\n",
    "    name = data_education['results']['bindings'][l]['name']['value']\n",
    "    abstract =  data_education['results']['bindings'][l]['abstract']['value']\n",
    "    ei_name.append(name)\n",
    "    ei_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# Lake\n",
    "lake_name = []\n",
    "lake_abstract = []\n",
    "\n",
    "length_lake = len(data_lake['results']['bindings'])\n",
    "\n",
    "for m in range(length_lake):\n",
    "    name = data_lake['results']['bindings'][m]['name']['value']\n",
    "    abstract =  data_lake['results']['bindings'][m]['abstract']['value']\n",
    "    lake_name.append(name)\n",
    "    lake_abstract.append(abstract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "person = ['Person']*length_p\n",
    "person_df = pd.DataFrame(\n",
    "    {'Names': person_names,\n",
    "     'Abstract': person_abstracts,\n",
    "     'Class': person\n",
    "    })\n",
    "    \n",
    "city = ['City']*length_c\n",
    "city_df = pd.DataFrame(\n",
    "    {'Names': city_name,\n",
    "     'Abstract': city_abstract,\n",
    "     'Class': city\n",
    "    })\n",
    "\n",
    "celestialbody = ['CelestialBody']*length_cb\n",
    "cb_df = pd.DataFrame(\n",
    "        {'Names': cb_name,\n",
    "        'Abstract': cb_abstract,\n",
    "        'Class': celestialbody})\n",
    "\n",
    "education = ['EducationalInstitution']*length_ei\n",
    "ei_df = pd.DataFrame(\n",
    "        {'Names': ei_name,\n",
    "        'Abstract': ei_abstract,\n",
    "        'Class': education})\n",
    "\n",
    "lake = ['Lake']*length_lake\n",
    "lake_df = pd.DataFrame(\n",
    "        {'Names': lake_name,\n",
    "        'Abstract': lake_abstract,\n",
    "        'Class': lake})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [cb_df, person_df, ei_df, lake_df, city_df]   \n",
    "\n",
    "whole_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split dataframe into testing and training: (random split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(whole_df, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Separate dataframes into data(X) and class labels(y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:,[1]]\n",
    "y_test = test.iloc[:,[1]]\n",
    "\n",
    "X_train = train.iloc[:,[0]]\n",
    "X_test = test.iloc[:,[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "hash_train = hash_vectorizer.fit_transform(X_train.Abstract)\n",
    "hash_test = hash_vectorizer.transform(X_test.Abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.Abstract)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.Abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "rfclf = RandomForestClassifier()\n",
    "rfclf.fit(hash_train, y_train)\n",
    "\n",
    "pred = rfclf.predict(hash_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "#cm = metrics.confusion_matrix(y_test, pred, labels=['City', 'Person'])\n",
    "#plot_confusion_matrix(cm, classes=['City', 'Person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "rfclf = RandomForestClassifier()\n",
    "rfclf.fit(tfidf_train, y_train)\n",
    "\n",
    "pred = rfclf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.993\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(10, weights = 'distance')\n",
    "clf.fit(hash_train, y_train)\n",
    "\n",
    "pred = clf.predict(hash_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.976\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(100, weights = 'uniform')\n",
    "clf.fit(tfidf_train, y_train)\n",
    "\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "sgdclf = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\")\n",
    "sgdclf.fit(hash_train, y_train)\n",
    "\n",
    "pred = sgdclf.predict(hash_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "sgdclf = SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\")\n",
    "sgdclf.fit(tfidf_train, y_train)\n",
    "\n",
    "pred = sgdclf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for those examples that were classified wrong: (5 datasets: Actor/Person, city, celestialBody, educationalInstitution, Lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct lists, so it's easy accessibel with indices.\n",
    "classes = []\n",
    "for e in y_test.Class:\n",
    "    classes.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "for a in test.Abstract:\n",
    "    abstracts.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how often each of the test abstracts were mistaken for the wrong class.\n",
    "p_c = 0\n",
    "p_cb = 0\n",
    "p_ei = 0\n",
    "p_l = 0\n",
    "c_p = 0\n",
    "c_cb = 0\n",
    "c_ei = 0\n",
    "c_l = 0\n",
    "cb_c = 0\n",
    "cb_p = 0\n",
    "cb_ei = 0\n",
    "cb_l = 0\n",
    "ei_c = 0\n",
    "ei_p = 0\n",
    "ei_cb= 0\n",
    "ei_l = 0\n",
    "l_p = 0\n",
    "l_c = 0\n",
    "l_cb = 0\n",
    "l_ei = 0\n",
    "false = []\n",
    "for i in range(len(classes)):\n",
    "    cl = classes[i]  # real label \n",
    "    pr = pred[i]  # predicted label\n",
    "    \n",
    "    # How often is which class confused with which other class?\n",
    "    if cl != pr:\n",
    "        false.append(i)  # create List with all indizees that were classified wrong.\n",
    "        if cl == 'Person':\n",
    "            if pr == 'City':\n",
    "                p_c+=1\n",
    "            elif pr == 'CelestialBody':\n",
    "                p_cb+=1\n",
    "            elif pr == 'EducationalInstitution':\n",
    "                p_ei+=1\n",
    "            elif pr == 'Lake':\n",
    "                p_l+=1\n",
    "        elif cl == 'City':\n",
    "            if pr == 'Person':\n",
    "                c_p+=1\n",
    "            elif pr == 'CelestialBody':\n",
    "                c_cb+=1\n",
    "            elif pr == 'EducationalInstitution':\n",
    "                c_ei+=1\n",
    "            elif pr == 'Lake':\n",
    "                c_l+=1\n",
    "        elif cl == 'CelestialBody':\n",
    "            if pr == 'City':\n",
    "                cb_c+=1\n",
    "            elif pr == 'Person':\n",
    "                cb_p+=1\n",
    "            elif pr == 'EducationalInstitution':\n",
    "                cb_ei+=1\n",
    "            elif pr == 'Lake':\n",
    "                cb_l+=1\n",
    "        elif cl == 'EducationalInstitution':\n",
    "            if pr == 'City':\n",
    "                ei_c+=1\n",
    "            elif pr == 'CelestialBody':\n",
    "                ei_cb+=1\n",
    "            elif pr == 'Person':\n",
    "                ei_p+=1\n",
    "            elif pr == 'Lake':\n",
    "                ei_l+=1\n",
    "        elif cl == 'Lake':\n",
    "            if pr == 'City':\n",
    "                l_c+=1\n",
    "            elif pr == 'CelestialBody':\n",
    "                l_cb+=1\n",
    "            elif pr == 'Person':\n",
    "                l_p+=1\n",
    "            elif pr == 'EducationalInstitution':\n",
    "                l_ei+=1\n",
    "        else:\n",
    "            print('Wrong Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look for explicit example\n",
    "# for a in range(len(abstracts)):\n",
    "#    if abstracts[a] == \"Heather Leigh West is a New York City based American recording artist best known for her work in house music.\":\n",
    "#        print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "print(len(false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstrcat: \n",
      "Colico is a city in the province of Lecco, Lombardy, Italy. It is situated on the northern arm of Lake Como, where the river Adda enters the lake. Colico is the most important city in the northern part of Lake Como, which is often identified as its Colico branch. Colico is a local transport hub, with boats to Como and Lecco, as well as trains and roads to Milan (via the eastern shore of the lake, Lecco and Brianza), to Chiavenna, and eastwards to Bolzano, via Passo dello Stelvio. The Piona Abbey is located in the communal territory, in the Olgiasca peninsula.\n",
      "\n",
      "is: City\n",
      "labeled as: Lake\n"
     ]
    }
   ],
   "source": [
    "# look at abstracts, that were not classified correctly\n",
    "ex = false[5]\n",
    "print('Abstrcat: \\n' + abstracts[ex] + '\\n')\n",
    "print('is: ' + classes[ex])\n",
    "print('labeled as: '+ pred[ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class = Person\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "\n",
      "\n",
      "Class = City\n",
      "5\n",
      "0\n",
      "15\n",
      "3\n",
      "\n",
      "\n",
      "Class = CelestialBody\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "Class = EducationalInstitute\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "Class = lake\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "City-Person\n",
      "8\n",
      "Person-CB\n",
      "1\n",
      "Person-ei\n",
      "4\n",
      "Person-Lake\n",
      "2\n",
      "City_cb\n",
      "0\n",
      "city-ei\n",
      "17\n",
      "city-lake\n",
      "6\n",
      "cb-lake\n",
      "0\n",
      "ei-cb\n",
      "0\n",
      "ei-lake\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Class = Person')\n",
    "print(p_c)\n",
    "print(p_cb)\n",
    "print(p_ei)\n",
    "print(p_l)\n",
    "print('\\n')\n",
    "\n",
    "print('Class = City')\n",
    "print(c_p)\n",
    "print(c_cb)\n",
    "print(c_ei)\n",
    "print(c_l)\n",
    "print('\\n')\n",
    "\n",
    "print('Class = CelestialBody')\n",
    "print(cb_p)\n",
    "print(cb_c)\n",
    "print(cb_ei)\n",
    "print(cb_l)\n",
    "print('\\n')\n",
    "\n",
    "print('Class = EducationalInstitute')\n",
    "print(ei_p)\n",
    "print(ei_c)\n",
    "print(ei_cb)\n",
    "print(ei_l)\n",
    "print('\\n')\n",
    "\n",
    "print('Class = lake')\n",
    "print(l_p)\n",
    "print(l_c)\n",
    "print(l_cb)\n",
    "print(l_ei)\n",
    "print('\\n')\n",
    "\n",
    "print('City-Person')\n",
    "print(p_c + c_p)\n",
    "\n",
    "print('Person-CB')\n",
    "print(p_cb + cb_p)\n",
    "\n",
    "print('Person-ei')\n",
    "print(ei_p+p_ei)\n",
    "\n",
    "print('Person-Lake')\n",
    "print(p_l+l_p)\n",
    "\n",
    "print('City_cb')\n",
    "print(c_cb+cb_c)\n",
    "\n",
    "print('city-ei')\n",
    "print(c_ei+ei_c)\n",
    "\n",
    "print('city-lake')\n",
    "print(c_l+l_c)\n",
    "\n",
    "print('cb-lake')\n",
    "print(cb_l+l_cb)\n",
    "\n",
    "print('ei-cb')\n",
    "print(cb_ei + ei_cb)\n",
    "\n",
    "print('ei-lake')\n",
    "print(ei_l+l_ei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
