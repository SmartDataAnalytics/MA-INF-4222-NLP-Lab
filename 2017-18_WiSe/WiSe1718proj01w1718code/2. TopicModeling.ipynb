{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load json files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#from pprint import pprint\n",
    "\n",
    "data_actor = json.load(open('actor.json'))\n",
    "data_city = json.load(open('city.json'))\n",
    "data_celestialbody = json.load(open('celestialbody.json'))\n",
    "data_education = json.load(open('educationalInstitution.json'))\n",
    "data_lake = json.load(open('lake.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create list with abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person(Actor)\n",
    "\n",
    "person_names = []\n",
    "person_abstracts = []\n",
    "\n",
    "length_p = len(data_actor['results']['bindings'])\n",
    "\n",
    "for i in range(length_p):\n",
    "    name = data_actor['results']['bindings'][i]['name']['value']\n",
    "    abstract = data_actor['results']['bindings'][i]['abstract']['value']\n",
    "    person_names.append(name.split())\n",
    "    person_abstracts.append(abstract)\n",
    "\n",
    "\n",
    "# City\n",
    "city_name = []\n",
    "city_abstract = []\n",
    "\n",
    "length_c = len(data_city['results']['bindings'])\n",
    "\n",
    "for j in range(length_c):\n",
    "    name = data_city['results']['bindings'][j]['name']['value']\n",
    "    abstract =  data_city['results']['bindings'][j]['abstract']['value']\n",
    "    city_name.append(name)\n",
    "    city_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# CelestialBody\n",
    "cb_name = []\n",
    "cb_abstract = []\n",
    "\n",
    "length_cb = len(data_celestialbody['results']['bindings'])\n",
    "\n",
    "for k in range(length_cb):\n",
    "    name = data_celestialbody['results']['bindings'][k]['name']['value']\n",
    "    abstract =  data_celestialbody['results']['bindings'][k]['abstract']['value']\n",
    "    cb_name.append(name)\n",
    "    cb_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# EducationalInstitution\n",
    "ei_name = []\n",
    "ei_abstract = []\n",
    "\n",
    "length_ei = len(data_education['results']['bindings'])\n",
    "\n",
    "for l in range(length_ei):\n",
    "    name = data_education['results']['bindings'][l]['name']['value']\n",
    "    abstract =  data_education['results']['bindings'][l]['abstract']['value']\n",
    "    ei_name.append(name)\n",
    "    ei_abstract.append(abstract)\n",
    "    \n",
    "    \n",
    "# Lake\n",
    "lake_name = []\n",
    "lake_abstract = []\n",
    "\n",
    "length_lake = len(data_lake['results']['bindings'])\n",
    "\n",
    "for m in range(length_lake):\n",
    "    name = data_lake['results']['bindings'][m]['name']['value']\n",
    "    abstract =  data_lake['results']['bindings'][m]['abstract']['value']\n",
    "    lake_name.append(name)\n",
    "    lake_abstract.append(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. combine all lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need abstracts.\n",
    "all_docs = person_abstracts + city_abstract + cb_abstract + ei_abstract + lake_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/eva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Add all names as well as 'also' to stopwords, as they do not characterize a topic.\n",
    "names = person_names + city_name + cb_name + ei_name + lake_name\n",
    "all_names = [val for sublist in names for val in sublist]\n",
    "\n",
    "stopword.add('also')\n",
    "for name in all_names:\n",
    "    stopword.add(name.lower())\n",
    "    stopword.add('lee')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    punc_free = ''.join(ch for ch in doc.lower() if ch not in punctuation)\n",
    "    stop_free = \" \".join([word for word in punc_free.split() if word not in stopword])  \n",
    "    normalized = \" \".join(lemma.lemmatize(wordb) for wordb in stop_free.split())  \n",
    "    return normalized\n",
    "\n",
    "        \n",
    "doc_clean = [clean(doc).split() for doc in all_docs]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Create the term dictionary of abstracts, every unique term is assigned to an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Convert list of abstracts into Document Term Matrix -> every word -> tupel (wird id, word frequency)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train the LDA model on tmatrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.018*\"province\" + 0.013*\"county\" + 0.013*\"norway\" + 0.010*\"lough\" + 0.010*\"district\" + 0.009*\"region\" + 0.008*\"city\" + 0.007*\"quechua\" + 0.007*\"qucha\" + 0.007*\"peru\"'), (1, '0.013*\"asteroid\" + 0.009*\"year\" + 0.009*\"galaxy\" + 0.008*\"constellation\" + 0.007*\"approximately\" + 0.007*\"discovered\" + 0.007*\"magnitude\" + 0.006*\"surface\" + 0.005*\"system\" + 0.005*\"named\"'), (2, '0.016*\"chinese\" + 0.012*\"film\" + 0.009*\"actor\" + 0.009*\"actress\" + 0.008*\"known\" + 0.008*\"name\" + 0.006*\"china\" + 0.006*\"role\" + 0.006*\"best\" + 0.005*\"japanese\"'), (3, '0.058*\"school\" + 0.018*\"high\" + 0.017*\"university\" + 0.014*\"college\" + 0.012*\"student\" + 0.009*\"located\" + 0.008*\"public\" + 0.007*\"education\" + 0.006*\"state\" + 0.006*\"year\"'), (4, '0.015*\"river\" + 0.015*\"located\" + 0.015*\"area\" + 0.013*\"reservoir\" + 0.013*\"water\" + 0.011*\"county\" + 0.011*\"city\" + 0.009*\"state\" + 0.008*\"km\" + 0.008*\"dam\"')]\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
